<!DOCTYPE html><html lang="zh-cn"><head><title>基于Java的爬虫框架WebCollector</title><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta name="viewport" content="width=device-width, initial-scale=0.5"><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"><link rel="manifest" href="/site.webmanifest"><link rel="stylesheet" href="/css/highlight/xcode.min.css"><link rel="stylesheet" href="/css/bootstrap/bootstrap-tooltips.css"><link rel="stylesheet" href="/css/post.css"><script src="/js/jquery.min.js"></script><meta name="generator" content="Hexo 6.3.0"><link rel="alternate" href="/atom.xml" title="smallyu的博客" type="application/atom+xml">
</head><body><script>if (/mobile/i.test(navigator.userAgent) || /android/i.test(navigator.userAgent)) {
  document.body.classList.add('mobile')
  var navbar = document.querySelector('nav.navbar');
  if (navbar) {
    navbar.classList.remove('navbar-fixed-top');
  }
}
</script><div><div class="inner"><h1>基于Java的爬虫框架WebCollector</h1><div class="time">2019-08-10</div><div class="title-margin"></div><p>Long, Long Ago，网络上出现大量Python爬虫教程，各种培训班借势宣扬Python，近几年又将噱头转向人工智能。爬虫是一个可以简单也可以复杂的概念，就好比建造狗屋和建筑高楼大厦都是在搞工程。</p>
<p>由于工作的缘故，我需要使用WebCollector爬取一些网页上的数据。其实宏观上，爬虫无非就是访问页面文件，把需要的数据提取出来，然后把数据储存到数据库里。难点往往在于，一是目标网站的反爬策略，这是让人比较无奈的斗智斗勇的过程；二是目标网页数量大、类型多，如何制定有效的数据爬取和数据分析方案。</p>
<h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><p>这是一张简略的概念图，受屏幕宽度限制，可能无法看清内容，请在新标签页打开图片，或者直接点击 <a href="/2019/08/10/%E5%9F%BA%E4%BA%8EJava%E7%9A%84%E7%88%AC%E8%99%AB%E6%A1%86%E6%9E%B6WebCollector/WebCollector.png">这里</a>。这张图片并不是完美的，甚至还包含不完全正确的实现方式，具体内容会在后面阐述。</p>
<img src="WebCollector.png" width="95%" height="100%">

<p>我将目标网页分为4种类型：</p>
<ol>
<li>静态的网页文档，curl就可以加载到</li>
<li>需要自定义HTTP请求的页面，比如由POST请求得到的搜索结果页面，或者需要使用Cookie进行鉴权的页面</li>
<li>页面中包含由JavaScript生成的数据，而我们需要的正是这部分数据。由于js是加载后才执行的，就像CSS加载后由浏览器进行渲染一样，这样的数据无法直接得到</li>
<li>页面中包含由JavaScript生成的数据，且需要自定义HTTP请求的页面</li>
</ol>
<h3 id="测试环境"><a href="#测试环境" class="headerlink" title="测试环境"></a>测试环境</h3><p>为了便于测试，在本地使用Node.js启动一个简单的服务器，用于接收请求，并返回一个页面作为响应。server.js的内容如下：</p>
<pre><code class="JavaScript">var http = require(&#39;http&#39;)
var fs = require(&#39;fs&#39;)
var server = http.createServer((req,res) =&gt; &#123;
  // 返回页面内容
  fs.readFile(&#39;./index.html&#39;, &#39;utf-8&#39;, (err,data) =&gt; &#123;
    res.end(data);
  &#125;);
  // 打印请求中的Cookie信息
  console.log(req.headers.cookie)
&#125;)
server.listen(9000) 
</code></pre>
<p>index.html的内容更加简单，只包含一个title和一个p标签：</p>
<pre><code class="HTML">&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;head&gt;
    &lt;title&gt;This is a title&lt;/title&gt;
&lt;/head&gt;
&lt;body&gt;

&lt;/body&gt;
&lt;/html&gt;
</code></pre>
<h3 id="静态页面"><a href="#静态页面" class="headerlink" title="静态页面"></a>静态页面</h3><p>这是一个最简版的爬虫程序，在构造方法中调用父类的有参构造方法，同时添加url到待爬取队列中。visit是消费者，每一个url请求都会进入这个方法被处理。</p>
<pre><code class="Java">public class StaticDocs extends BreadthCrawler &#123;

    public StaticDocs(String crawlPath, boolean autoParse) &#123;
        super(crawlPath, autoParse);
        this.addSeed(&quot;http://127.0.0.1:9000/&quot;);
    &#125;

    @Override
    public void visit(Page page, CrawlDatums next) &#123;
        System.out.println(page.doc().title();
        // This is a title
    &#125;

    public static void main(String[] args) throws Exception &#123;
        StaticDocs crawler = new StaticDocs(&quot;crawl&quot;, true);
        crawler.start(1);
    &#125;
&#125;
</code></pre>
<h3 id="Cookie鉴权"><a href="#Cookie鉴权" class="headerlink" title="Cookie鉴权"></a>Cookie鉴权</h3><p>需要在header中带cookie请求同样简单，在构造方法中添加相应配置就可以，node.js的命令行会打印出cookie的内容：</p>
<pre><code class="Java">public CookieDocs(String crawlPath) &#123;
    super(crawlPath, true);

    // 设置请求插件
    setRequester(new OkHttpRequester() &#123;
        @Override
        public Request.Builder createRequestBuilder(CrawlDatum crawlDatum) &#123;
            return super.createRequestBuilder(crawlDatum)
                    .header(&quot;Cookie&quot;, &quot;name=smallyu&quot;);
        &#125;
    &#125;);

    this.addSeed(&quot;http://127.0.0.1:9000/&quot;);
&#125;

// name=smallyu
</code></pre>
<h3 id="JavaScript生成的数据"><a href="#JavaScript生成的数据" class="headerlink" title="JavaScript生成的数据"></a>JavaScript生成的数据</h3><p>测试js生成数据的情况需要做一点准备，修改index.html，在body标签中加入这样几行代码：</p>
<pre><code class="JavaScript">&lt;div id=&quot;content&quot;&gt;1&lt;/div&gt;
&lt;script&gt;
  document.getElementById(&#39;content&#39;).innerHTML = &#39;2&#39;
&lt;/script&gt;
</code></pre>
<p>可以预见，请求中直接返回的div内容是1，然后js经由浏览器执行，改变div的内容为2。访问静态页面的爬虫程序只能进行到第1步，也就是直接获取请求返回的内容。修改StaticDocs.java的visit方法，打印出div的内容看一下，可以确信是1：</p>
<pre><code class="Java">System.out.println(page.select(&quot;div&quot;).text());
// 1
</code></pre>
<p>这是一个官方提供的Demo，用于获取js生成的数据。WebCollector依赖于Selenium，使用HtmlUnitDriver运行js：</p>
<pre><code class="Java">public class JsDocs &#123;
    public static void main(String[] args) throws Exception &#123;
        Executor executor = (CrawlDatum datum, CrawlDatums next) -&gt; &#123;
            HtmlUnitDriver driver = new HtmlUnitDriver();
            driver.setJavascriptEnabled(true);

            driver.get(datum.url());

            WebElement divEle = driver.findElement(By.id(&quot;content&quot;));
            System.out.println(divEle.getText());
            // 2
        &#125;;

        //创建一个基于伯克利DB的DBManager
        DBManager manager = new RocksDBManager(&quot;crawl&quot;);
        //创建一个Crawler需要有DBManager和Executor
        Crawler crawler = new Crawler(manager, executor);
        crawler.addSeed(&quot;http://127.0.0.1:9000/&quot;);
        crawler.start(1);
    &#125;
&#125;
</code></pre>
<p>如果你看过WebCollector的主页，一定可以注意到这个Demo和其他Demo的明显不同。在不需要js生成的数据时，新建的类继承自BreadthCrawler，而BreadthCrawler继承自AutoParseCrawler，AutoParseCrawler又继承自Crawler。现在获取js数据的Demo，直接跳过BreadthCrawler和AutoParseCrawler，实例化了Crawler。</p>
<img src="uml.png" width="50%" height="100%">

<p>为什么要这样做呢？再次强调，这是官方提供的Demo。</p>
<h3 id="Cookie鉴权后JavaScript生成的数据"><a href="#Cookie鉴权后JavaScript生成的数据" class="headerlink" title="Cookie鉴权后JavaScript生成的数据"></a>Cookie鉴权后JavaScript生成的数据</h3><p>根据官方提供的用例，显然是无法设置cookie的，因为Crawler类并没有提供自定义Header的方法。这个自定义Header的方法继承自AutoParseCrawler类。那么如何做到既可以添加Cookie又可以使用HtmlUnitDriver？</p>
<p>其实结果很简单，我在看过WebCollector的代码后发现AutoParseCrawler实现了Executor接口，并且在构造方法中将this赋值给了父类的executor。也就是说，AutoParseCrawler本身就是一个Executor。下面的代码用以表示它们的关系：</p>
<pre><code class="Java">public class Crawler &#123;
    protected Executor executor;

    public Crawler(DBManager dbManager, Executor executor) &#123;
        // ...
    &#125;
&#125;

public class AutoParseCrawler extends Crawler implements Executor &#123;
    public AutoParseCrawler(boolean autoParse) &#123;
        // 这里的executor指向父类
        this.executor = this;
    &#125;
&#125;
</code></pre>
<p>new Crawler时传入一个executor，相当于直接new一个AutoParseCrawler。BreadthCrawler继承自AutoParseCrawler，所以BreadthCrawler本身也是个Executor。再看官方关于自定义Cookie的Demo，如何在其中使用HtmlUnitDriver呢？重写Executor的execute方法。</p>
<p>所以，在定义cookie后获取js生成的数据，使用继承BreadthCrawler的类，然后重写execute就可以。这是一个完整的Demo：</p>
<pre><code class="Java">/**
 * @author smallyu
 * @date 2019.08.11 12:18
 */
public class JsWithCookieDocs extends BreadthCrawler &#123;

    public JsWithCookieDocs(String crawlPath) &#123;
        super(crawlPath, true);

        // 设置请求插件
        setRequester(new OkHttpRequester() &#123;
            @Override
            public Request.Builder createRequestBuilder(CrawlDatum crawlDatum) &#123;
                return super.createRequestBuilder(crawlDatum)
                        .header(&quot;Cookie&quot;, &quot;name=smallyu&quot;);
            &#125;
        &#125;);

        this.addSeed(&quot;http://127.0.0.1:9000/&quot;);
    &#125;

    // 直接重写execute即可
    @Override
    public void execute(CrawlDatum datum, CrawlDatums next) throws Exception &#123;
        super.execute(datum, next);

        HtmlUnitDriver driver = new HtmlUnitDriver();
        driver.setJavascriptEnabled(true);

        driver.get(datum.url());

        WebElement divEle = driver.findElement(By.id(&quot;content&quot;));
        System.out.println(divEle.getText());
        // 2
        // 同时，node.js的命令行中打印出cookie内容
    &#125;

    // 重写execute就不需要visit了
    public void visit(Page page, CrawlDatums crawlDatums) &#123;&#125;

    public static void main(String[] args) throws Exception &#123;
        JsWithCookieDocs crawler = new JsWithCookieDocs(&quot;crawl&quot;);
        crawler.start(1);
    &#125;
&#125;
</code></pre>
<h3 id="外部代理"><a href="#外部代理" class="headerlink" title="外部代理"></a>外部代理</h3><p>也许还没有结束。在一开始概述的图片上，同时定义cookie以及获取js生成的数据，实现方式是内部Selenium + 外部browsermob-proxy。假设没有上述重写execute的方法（官方也确实没有提供类似的Demo），该如何实现想要的效果？一种实践是本地启动一个代理，给代理设置好cookie，然后让Selenium的WebDriver通过代理访问目标页面，就可以在带header的情况下拿到js生成的数据。这是在JsDocs.java的基础上，使用代理的完整实现：</p>
<pre><code class="Java">public class JsWithProxyDocs &#123;
    public static void main(String[] args) throws Exception &#123;
        Executor executor = (CrawlDatum datum, CrawlDatums next) -&gt; &#123;

            // 启动一个代理
            BrowserMobProxy proxy = new BrowserMobProxyServer();
            proxy.start(0);
            // 添加header
            proxy.addHeader(&quot;Cookie&quot; , &quot;name=smallyu&quot;);

            // 实例化代理对象
            Proxy seleniumProxy = ClientUtil.createSeleniumProxy(proxy);
            // 由代理对象生成capabilities
            DesiredCapabilities capabilities = new DesiredCapabilities();
            capabilities.setCapability(CapabilityType.PROXY, seleniumProxy);
            // 内置，必须设置
            capabilities.setBrowserName(&quot;htmlunit&quot;);

            // 使用capabilities实例化HtmlUnitDriver
            HtmlUnitDriver driver = new HtmlUnitDriver(capabilities);
            driver.setJavascriptEnabled(true);

            driver.get(datum.url());

            WebElement divEle = driver.findElement(By.id(&quot;content&quot;));
            System.out.println(divEle.getText());   // 2
        &#125;;

        //创建一个Crawler需要有DBManager和Executor
        Crawler crawler = new Crawler(new RocksDBManager(&quot;crawl&quot;), executor);
        crawler.addSeed(&quot;http://127.0.0.1:9000/&quot;);
        crawler.start(1);
    &#125;
&#125;
</code></pre>
<h3 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h3><p>对于WebCollector我已经没有兴趣了解更多，倒是在注意到框架的包名<code>cn.edu.hfut</code>后有种豁然开朗的感觉。凌乱的代码风格，随处可见不知所以的注释，毫无设计美感的代码架构，倒也符合国内不知名大学的开源软件水平，距离工业级的框架，可能还需要N个指数倍东的时间。至于使用过程中遇到depth含义不明、线程非法结束、next.add失效等问题，就这样吧，也在情理之中，整个框架都像是赶工的结果，或者说是学生们拿来练手的项目。我在WebCollector的Github上RP了关于重写execute的问题，从开发者回复的只言片语中，我怀疑开源者自己都没有把里面的东西搞清楚 :P</p>
</div></div></body><script src="/js/highlight.min.js"></script><script src="/js/main.js"></script><script src="/js/bootstrap/bootstrap.min.js"></script><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-5JRBZ6P1W3"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-5JRBZ6P1W3');</script></html>